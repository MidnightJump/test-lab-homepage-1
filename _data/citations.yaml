# EIT-NLP实验室研究成果 - 手动管理

# ========== 2024年重点论文 ==========

- id: arxiv:2404.14122
  title: "Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?"
  authors:
  - "**Dawei Zhu**"
  - "Pinzhen Chen"
  - "Miaoran Zhang"
  - "Barry Haddow"
  - "**Xiaoyu Shen**"
  - "Dietrich Klakow"
  publisher: "arXiv preprint"
  date: '2024-04-22'
  link: https://arxiv.org/pdf/2404.14122
  type: paper
  description: "重新审视多语言机器翻译成功的关键因素。研究发现，大语言模型在仅微调32对平行句子的情况下就表现出了强大的翻译能力，挑战了传统对大规模数据的依赖认知。"
  image: images/papers/placeholder.png
  buttons:
  - type: source
    link: https://github.com/uds-lsv/mt-sft
    text: "代码仓库"
  - type: website
    link: https://arxiv.org/pdf/2404.14122
    text: "PDF下载"
  tags:
  - "机器翻译"
  - "大语言模型"
  - "少样本学习"
  repo: uds-lsv/mt-sft

- id: arxiv:2410.06554
  title: "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models"
  authors:
  - "**Yanjun Chen**"
  - "**Dawei Zhu**"
  - "Yirong Sun"
  - "Xinghao Chen"
  - "Wei Zhang"
  - "**Xiaoyu Shen**"
  publisher: "arXiv preprint"
  date: '2024-10-09'
  link: https://arxiv.org/abs/2410.06554
  type: paper
  description: "揭示了人类反馈强化学习中的准确性悖论：使用中等准确率奖励模型训练的语言模型表现竟然优于使用高准确率奖励模型的语言模型，为RLHF研究开辟了新方向。"
  image: images/papers/placeholder.png
  buttons:
  - type: source
    link: https://github.com/EIT-NLP/AccuracyParadox-RLHF
    text: "代码仓库"
  - type: website
    link: https://arxiv.org/abs/2410.06554
    text: "arXiv论文"
  tags:
  - "强化学习"
  - "人类反馈"
  - "奖励模型"
  repo: EIT-NLP/AccuracyParadox-RLHF

- id: arxiv:2407.17011
  title: "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism"
  authors:
  - "**Anhao Zhao**"
  - "Fanghua Ye"
  - "Jinlan Fu"
  - "**Xiaoyu Shen**"
  publisher: "arXiv preprint"
  date: '2024-07-17'
  link: https://arxiv.org/pdf/2407.17011
  type: paper
  description: "提出了一个二维坐标系统，将上下文学习的两种相互矛盾的观点统一在一个系统框架中，通过感知和认知两个正交变量解释ICL的行为机制。"
  image: images/papers/placeholder.png
  buttons:
  - type: source
    link: https://github.com/EIT-NLP/2D-Coordinate-System-for-ICL
    text: "代码仓库"
  - type: website
    link: https://arxiv.org/pdf/2407.17011
    text: "PDF下载"
  tags:
  - "上下文学习"
  - "大语言模型"
  - "认知机制"
  repo: EIT-NLP/2D-Coordinate-System-for-ICL

- id: arxiv:2410.06765
  title: "To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models"
  authors:
  - "**Junyan Lin**"
  - "**Haoran Chen**"
  - "**Dawei Zhu**"
  - "**Xiaoyu Shen**"
  publisher: "arXiv preprint"
  date: '2024-10-09'
  link: http://arxiv.org/abs/2410.06765
  type: paper
  description: "系统地探讨了连接器对多模态大语言模型性能的影响。我们将连接器分为保留特征型和压缩特征型两类，为MLLM架构设计提供重要指导。"
  image: images/papers/placeholder.png
  buttons:
  - type: source
    link: https://github.com/EIT-NLP/Connector-Selection-for-MLLM
    text: "代码仓库"
  - type: website
    link: http://arxiv.org/abs/2410.06765
    text: "arXiv论文"
  tags:
  - "多模态学习"
  - "大语言模型"
  - "架构优化"
  repo: EIT-NLP/Connector-Selection-for-MLLM

- id: arxiv:2410.04691
  title: "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"
  authors:
  - "**Qingyu Yin**"
  - "Xuzheng He"
  - "Luoao Deng"
  - "Chak Tou Leong"
  - "Fan Wang"
  - "Yanzhao Yan"
  - "**Xiaoyu Shen**"
  - "Qiang Zhang"
  publisher: "arXiv preprint"
  date: '2024-10-04'
  link: https://arxiv.org/abs/2410.04691
  type: paper
  description: "提出了一个反直觉的发现：对于隐式模式任务，ICL能够显著优于微调捕捉这些模式，尽管微调使用了比ICL多数千倍的训练样本。"
  image: images/papers/placeholder.png
  buttons:
  - type: source
    link: https://github.com/MikaStars39/ICLvsFinetune
    text: "代码仓库"
  - type: website
    link: https://arxiv.org/abs/2410.04691
    text: "arXiv论文"
  tags:
  - "上下文学习"
  - "微调"
  - "隐式模式"
  repo: MikaStars39/ICLvsFinetune

- id: arxiv:2406.18134
  title: "Assessing 'Implicit' Retrieval Robustness of Large Language Models"
  authors:
  - "**Xiaoyu Shen**"
  - "Rexhina Blloshmi"
  - "**Dawei Zhu**"
  - "Jiahuan Pei"
  - "Wei Zhang"
  publisher: "arXiv preprint"
  date: '2024-06-18'
  link: https://arxiv.org/pdf/2406.18134
  type: paper
  description: "评估了不同大型语言模型的'隐式'检索稳健性，研究表明混合使用真实数据和干扰上下文进行微调能够显著增强模型应对检索不准确的稳健性。"
  image: images/papers/placeholder.png
  buttons:
  - type: website
    link: https://arxiv.org/pdf/2406.18134
    text: "PDF下载"
  tags:
  - "检索增强生成"
  - "稳健性"
  - "大语言模型"

# ========== 2023年论文 ==========

- id: arxiv:2309.16289
  title: "LawBench: Benchmarking Legal Knowledge of Large Language Models"
  authors:
  - "Zhiwei Fei"
  - "**Xiaoyu Shen**"
  - "**Dawei Zhu**"
  - "Fengzhe Zhou"
  - "Zhuo Han"
  - "Alan Huang"
  - "Songyang Zhang"
  - "Kai Chen"
  - "Zhixin Yin"
  - "Zongwen Shen"
  - "Jidong Ge"
  - "Vincent Ng"
  publisher: "arXiv preprint"
  date: '2023-09-16'
  link: https://arxiv.org/pdf/2309.16289
  type: paper
  description: "提出了LawBench，这是第一个由20个任务组成的评估基准，旨在评估大型语言模型在处理中文法律相关任务中的表现。"
  image: images/papers/placeholder.png
  buttons:
  - type: source
    link: https://github.com/open-compass/LawBench/
    text: "代码仓库"
  - type: website
    link: https://arxiv.org/pdf/2309.16289
    text: "PDF下载"
  tags:
  - "法律知识"
  - "基准测试"
  - "大语言模型"
  repo: open-compass/LawBench

# ========== 早期论文 ==========

- id: doi:10.1109/TCBB.2018.2859380
  title: "Simulating the Large-Scale Erosion of Genomic Privacy Over Time"
  authors:
  - "Michael Backes"
  - "Pascal Berrang"
  - "Mathias Humbert"
  - "**Xiaoyu Shen**"
  - "Verena Wolf"
  publisher: "IEEE/ACM Transactions on Computational Biology and Bioinformatics"
  date: '2018-09-01'
  link: https://doi.org/10.1109/TCBB.2018.2859380
  type: paper
  description: "研究了基因组隐私随时间的大规模侵蚀现象，为生物信息学领域的隐私保护提供了重要理论基础。"
  tags:
  - "基因组隐私"
  - "生物信息学"
  - "隐私保护"

- id: doi:10.1016/j.datak.2018.05.007
  title: "A comprehensive study: Sentence compression with linguistic knowledge-enhanced gated neural network"
  authors:
  - "Yang Zhao"
  - "**Xiaoyu Shen**"
  - "Hajime Senuma"
  - "Akiko Aizawa"
  publisher: "Data & Knowledge Engineering"
  date: '2018-09-01'
  link: https://doi.org/10.1016/j.datak.2018.05.007
  type: paper
  description: "提出了基于语言学知识增强的门控神经网络进行句子压缩的综合研究，在保持语义完整性的同时实现有效的文本压缩。"
  tags:
  - "句子压缩"
  - "神经网络"
  - "语言学知识"
