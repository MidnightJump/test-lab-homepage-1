<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  






























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>首页 | EIT-NLP</title>

<link rel="icon" href="/test-lab-homepage-1/images/icon.png">

<meta name="title" content="首页">
<meta name="description" content="An engaging 1-3 sentence description of your lab.">

<meta property="og:title" content="首页">
<meta property="og:site_title" content="EIT-NLP">
<meta property="og:description" content="An engaging 1-3 sentence description of your lab.">
<meta property="og:url" content="https://midnightjump.github.io/test-lab-homepage-1">
<meta property="og:image" content="/test-lab-homepage-1/images/share.jpg">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="首页">
<meta property="twitter:description" content="An engaging 1-3 sentence description of your lab.">
<meta property="twitter:url" content="https://midnightjump.github.io/test-lab-homepage-1">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/test-lab-homepage-1/images/share.jpg">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "首页",
    "description": "An engaging 1-3 sentence description of your lab.",
    "headline": "首页",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/test-lab-homepage-1/images/icon.png" }
    },
    "url": "https://midnightjump.github.io/test-lab-homepage-1"
  }
</script>

<link rel="alternate" type="application/rss+xml" href="https://midnightjump.github.io/test-lab-homepage-1/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.7.0/css/all.css" rel="stylesheet" media="none" onload="this.removeAttribute('media'); this.onload = null;">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.7.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/test-lab-homepage-1/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/all.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/body.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/button.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/code.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/details.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/float.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/font.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/form.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/header.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/image.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/link.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/list.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/main.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/section.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/table.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/test-lab-homepage-1/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/test-lab-homepage-1/_scripts/anchors.js"></script>

  <script src="/test-lab-homepage-1/_scripts/dark-mode.js"></script>

  <script src="/test-lab-homepage-1/_scripts/fetch-tags.js"></script>

  <script src="/test-lab-homepage-1/_scripts/search.js"></script>

  <script src="/test-lab-homepage-1/_scripts/site-search.js"></script>

  <script src="/test-lab-homepage-1/_scripts/table-wrap.js"></script>

  <script src="/test-lab-homepage-1/_scripts/tooltip.js"></script>


</head>

  <body>
    








<header class="background" style="--image: url('/test-lab-homepage-1/images/background.jpg')" data-dark="true" data-big>
  <a href="/test-lab-homepage-1" class="home">
    
    <span class="logo">
      
      <img src="/test-lab-homepage-1/images/logo.png" alt="logo">
      
    </span>
    
    
    <span class="title-text" data-tooltip="Home">
      
      <span class="title">EIT-NLP</span>
      
      
    </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
    
    <a href="/test-lab-homepage-1/" data-tooltip="主页">
      首页
    </a>
    
    
    
    <a href="/test-lab-homepage-1/research/" data-tooltip="研究项目与成果">
      研究
    </a>
    
    
    
    <a href="/test-lab-homepage-1/team/" data-tooltip="课题组成员">
      团队
    </a>
    
    
    
    <a href="/test-lab-homepage-1/hiring/" data-tooltip="加入我们">
      招聘
    </a>
    
    
  </nav>
</header>
    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - filter out blank sections
-->






  
  
  

  <section class="background" data-size="page">
    <h1 id="宁波东方理工大学自然语言处理课题组">宁波东方理工大学自然语言处理课题组</h1>

<p>我们是<a href="https://www.eitech.edu.cn/">宁波东方理工大学（暂名）</a>自然语言处理课题组。东方理工大学坐落于我国东南沿海重要港口城市、历史文化名城宁波，致力于汇聚全球顶尖人才，培育高水平、创新型、国际化的新型研究型大学。</p>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  <background></background>
  <dark></dark>
  <size></size>
-->

<h2 id="新闻">新闻</h2>

<h3 id="2024">2024</h3>

<div class="card" data-style="small">
  <a href="http://arxiv.org/abs/2410.06765" aria-label="To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models" class="card-image">
    <img src="/test-lab-homepage-1/images/citations/2410_06765.jpg" alt="To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models" loading="lazy" onerror="this.src = '/test-lab-homepage-1/images/fallback.svg'; this.onerror = null;">
  </a>

  <div class="card-text">
    
      <a href="http://arxiv.org/abs/2410.06765" class="card-title">
        To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models
    </a>
    

    

    
      <p>
        近年来，多模态大型语言模型（MLLMs）引起了产业界和学术界的广泛关注。根据融合位置的不同，MLLMs分为外部融合和内部融合架构，其中外部融合架构占主导地位。本文系统地探讨了连接器对MLLM性能的影响。具体而言，我们将连接器分为保留特征型和压缩特征型两类。通过统一的分类标准，我们将来自三个综合基准数据集（MMBench、MME、SEED-Bench）的子任务划分为粗粒度感知、细粒度感知和推理三种任务类型。

      </p>
    

    
      


  <div class="tags" data-repo="EIT-NLP/Connector-Selection-for-MLLM">
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E5%A4%9A%E6%A8%A1%E6%80%81%E5%AD%A6%E4%B9%A0%22" class="tag" data-tooltip='Show items with the tag "多模态学习"'>
        多模态学习
      </a>
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%22" class="tag" data-tooltip='Show items with the tag "大语言模型"'>
        大语言模型
      </a>
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E6%9E%B6%E6%9E%84%E4%BC%98%E5%8C%96%22" class="tag" data-tooltip='Show items with the tag "架构优化"'>
        架构优化
      </a>
    
  </div>


    
  </div>
</div>

<div class="card" data-style="small">
  <a href="https://arxiv.org/abs/2410.06554" aria-label="The Accuracy Paradox in RLHF: When Better Reward Models Don t Yield Better Language Models" class="card-image">
    <img src="/test-lab-homepage-1/images/citations/2410_06554.png" alt="The Accuracy Paradox in RLHF: When Better Reward Models Don t Yield Better Language Models" loading="lazy" onerror="this.src = '/test-lab-homepage-1/images/fallback.svg'; this.onerror = null;">
  </a>

  <div class="card-text">
    
      <a href="https://arxiv.org/abs/2410.06554" class="card-title">
        The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models
    </a>
    

    

    
      <p>
        人类反馈强化学习显著提升了自然语言处理的效果，通过训练使语言模型更符合人类预期。本研究探讨了更强的奖励模型是否必然导致更优的语言模型表现。本文通过使用QA-FEEDBACK数据集和基于Longformer的奖励模型，针对相关性、事实性和完整性任务进行实验，揭示了一个令人惊讽的悖论：训练时使用中等准确率奖励模型的语言模型表现优于那些使用高准确率奖励模型的语言模型。

      </p>
    

    
      


  <div class="tags" data-repo="EIT-NLP/AccuracyParadox-RLHF">
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%22" class="tag" data-tooltip='Show items with the tag "强化学习"'>
        强化学习
      </a>
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%22" class="tag" data-tooltip='Show items with the tag "人类反馈"'>
        人类反馈
      </a>
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%22" class="tag" data-tooltip='Show items with the tag "奖励模型"'>
        奖励模型
      </a>
    
  </div>


    
  </div>
</div>

<div class="card" data-style="small">
  <a href="https://arxiv.org/abs/2410.04691" aria-label="Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning" class="card-image">
    <img src="/test-lab-homepage-1/images/citations/2410_04691.jpg" alt="Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning" loading="lazy" onerror="this.src = '/test-lab-homepage-1/images/fallback.svg'; this.onerror = null;">
  </a>

  <div class="card-text">
    
      <a href="https://arxiv.org/abs/2410.04691" class="card-title">
        Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning
    </a>
    

    

    
      <p>
        微调和上下文学习（ICL）是两种常用的为大型语言模型注入任务特定知识的方法。通常认为，微调由于可以基于训练数据调整模型的内部参数，在有足够样本的情况下会优于ICL。然而，本文提出了一个反直觉的发现：对于隐式模式任务，ICL能够显著优于微调捕捉这些模式。我们构建了多个包含隐式模式的数据集，例如通过奇偶性确定答案的序列或在计算中识别可约项。

      </p>
    

    
      


  <div class="tags" data-repo="MikaStars39/ICLvsFinetune">
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0%22" class="tag" data-tooltip='Show items with the tag "上下文学习"'>
        上下文学习
      </a>
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E5%BE%AE%E8%B0%83%22" class="tag" data-tooltip='Show items with the tag "微调"'>
        微调
      </a>
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E9%9A%90%E5%BC%8F%E6%A8%A1%E5%BC%8F%22" class="tag" data-tooltip='Show items with the tag "隐式模式"'>
        隐式模式
      </a>
    
  </div>


    
  </div>
</div>

<div class="card" data-style="small">
  <a href="https://arxiv.org/pdf/2407.17011" aria-label="Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism" class="card-image">
    <img src="/test-lab-homepage-1/images/citations/2407_17011.png" alt="Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism" loading="lazy" onerror="this.src = '/test-lab-homepage-1/images/fallback.svg'; this.onerror = null;">
  </a>

  <div class="card-text">
    
      <a href="https://arxiv.org/pdf/2407.17011" class="card-title">
        Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism
    </a>
    

    

    
      <p>
        大型语言模型（LLMs）展现了显著的上下文学习（ICL）能力，然而其背后的工作机制仍未被充分理解。现有研究对ICL提出了两种相互矛盾的观点：一种强调演示中相似样本的重要性，指出标签正确性和更多示例的重要性；另一种则将ICL归因于LLM固有的任务识别能力，认为标签正确性和示例数量并非关键。在本研究中，我们提出了一个二维坐标系统，将这两种观点统一在一个系统框架中。

      </p>
    

    
      


  <div class="tags" data-repo="EIT-NLP/2D-Coordinate-System-for-ICL">
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0%22" class="tag" data-tooltip='Show items with the tag "上下文学习"'>
        上下文学习
      </a>
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%22" class="tag" data-tooltip='Show items with the tag "大语言模型"'>
        大语言模型
      </a>
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E8%AE%A4%E7%9F%A5%E6%9C%BA%E5%88%B6%22" class="tag" data-tooltip='Show items with the tag "认知机制"'>
        认知机制
      </a>
    
  </div>


    
  </div>
</div>

<div class="card" data-style="small">
  <a href="https://arxiv.org/pdf/2406.18134" aria-label="Assessing Implicit Retrieval Robustness of Large Language Models" class="card-image">
    <img src="/test-lab-homepage-1/images/citations/2406_18134.jpg" alt="Assessing Implicit Retrieval Robustness of Large Language Models" loading="lazy" onerror="this.src = '/test-lab-homepage-1/images/fallback.svg'; this.onerror = null;">
  </a>

  <div class="card-text">
    
      <a href="https://arxiv.org/pdf/2406.18134" class="card-title">
        Assessing 'Implicit' Retrieval Robustness of Large Language Models
    </a>
    

    

    
      <p>
        检索增强生成（Retrieval-augmented generation，RAG）作为一种通过外部知识增强大型语言模型的框架，近年来备受关注。然而，其有效性在很大程度上依赖于模型的检索稳健性。如果模型缺乏检索稳健性，其性能将受限于检索器的准确性，当检索到的上下文与任务无关时，模型表现可能大幅下降。在本文中，我们评估了不同大型语言模型的’隐式’检索稳健性，要求它们直接输出最终答案，而不显式判断检索到的上下文的相关性。

      </p>
    

    
      


  <div class="tags">
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90%22" class="tag" data-tooltip='Show items with the tag "检索增强生成"'>
        检索增强生成
      </a>
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E7%A8%B3%E5%81%A5%E6%80%A7%22" class="tag" data-tooltip='Show items with the tag "稳健性"'>
        稳健性
      </a>
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%22" class="tag" data-tooltip='Show items with the tag "大语言模型"'>
        大语言模型
      </a>
    
  </div>


    
  </div>
</div>

<div class="card" data-style="small">
  <a href="https://arxiv.org/pdf/2404.14122" aria-label="Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice" class="card-image">
    <img src="/test-lab-homepage-1/images/citations/2404_14122.png" alt="Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice" loading="lazy" onerror="this.src = '/test-lab-homepage-1/images/fallback.svg'; this.onerror = null;">
  </a>

  <div class="card-text">
    
      <a href="https://arxiv.org/pdf/2404.14122" class="card-title">
        Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?
    </a>
    

    

    
      <p>
        传统上，多语言机器翻译的成功归因于训练数据的三个关键因素：大规模数据量、多样化的翻译方向以及高质量的数据。在当前微调大语言模型（LLMs）以进行翻译的实践中，我们重新审视了这些因素的重要性。我们发现，LLMs 在仅微调 32 对平行句子的情况下就表现出了强大的翻译能力，并且微调单一翻译方向能够实现多方向的翻译。

      </p>
    

    
      


  <div class="tags" data-repo="uds-lsv/mt-sft">
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%22" class="tag" data-tooltip='Show items with the tag "机器翻译"'>
        机器翻译
      </a>
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%22" class="tag" data-tooltip='Show items with the tag "大语言模型"'>
        大语言模型
      </a>
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E5%B0%91%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%22" class="tag" data-tooltip='Show items with the tag "少样本学习"'>
        少样本学习
      </a>
    
  </div>


    
  </div>
</div>

<h3 id="2023">2023</h3>

<div class="card" data-style="small">
  <a href="https://arxiv.org/pdf/2309.16289" aria-label="LawBench: Benchmarking Legal Knowledge of Large Language Models" class="card-image">
    <img src="/test-lab-homepage-1/images/citations/2309_16289.jpg" alt="LawBench: Benchmarking Legal Knowledge of Large Language Models" loading="lazy" onerror="this.src = '/test-lab-homepage-1/images/fallback.svg'; this.onerror = null;">
  </a>

  <div class="card-text">
    
      <a href="https://arxiv.org/pdf/2309.16289" class="card-title">
        LawBench: Benchmarking Legal Knowledge of Large Language Models
    </a>
    

    

    
      <p>
        我们提出了LawBench，这是第一个由20个任务组成的评估基准，旨在评估大型语言模型（LLMs）在处理中文法律相关任务中的表现。LawBench经过精心设计，能够从广泛接受的布卢姆认知分类法对应的三个认知层次上，精确评估LLMs的法律能力。利用LawBench，我们对21个热门LLMs进行了全面的评估，并首次对其实际表现进行了比较分析，揭示了它们的相对优势与劣势。

      </p>
    

    
      


  <div class="tags" data-repo="open-compass/LawBench">
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E6%B3%95%E5%BE%8B%E7%9F%A5%E8%AF%86%22" class="tag" data-tooltip='Show items with the tag "法律知识"'>
        法律知识
      </a>
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%22" class="tag" data-tooltip='Show items with the tag "基准测试"'>
        基准测试
      </a>
    
      <a href="/test-lab-homepage-1/?search=%22tag:%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%22" class="tag" data-tooltip='Show items with the tag "大语言模型"'>
        大语言模型
      </a>
    
  </div>


    
  </div>
</div>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  <background></background>
  <dark></dark>
  <size></size>
-->

<h2 id="关于我们">关于我们</h2>

<p>我们是 宁波东方理工大学（暂名）自然语言处理课题组。东方理工大学（暂名）坐落于我国东南沿海重要港口城市、历史文化名城宁波，以汇聚全球顶尖人才，孕育一所高水平、创新型、国际化的新型研究型大学为使命、短期内成为世界一流学术机构和国家重点战略学科引领者为目标、倡导平等、开放、自由的学术风气和引领变革性研究的新型非营利高端科研机构。</p>

<p>我们自然语言处理（NLP）课题组的研究方向与东方理工大学（暂名）的使命紧密契合，致力于推动变革性研究。我们的工作重点是通过创新的方法推进自然语言处理的发展，使机器能够深入理解、生成并推理人类语言。我们涵盖的研究领域广泛，包括信息检索、多模态学习以及链式推理等。课题组的核心目标是开发高效算法，在实现最先进性能的同时，最大限度减少计算和资源需求。我们优先减少对大规模人工标注的依赖，加快训练速度，降低推理成本，最终旨在构建强大且资源高效的模型，使其在各种应用中都能得到广泛应用并展现卓越效果。</p>

<p>想了解更多内容，欢迎关注我们的微信公众号：EIT-NLP。</p>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  <background></background>
  <dark></dark>
  <size></size>
-->
  </section>


    </main>
    


<footer class="background" style="--image: url('/test-lab-homepage-1/images/background.jpg')" data-dark="true" data-size="wide">
  <!--
    <div>
      Extra details like contact info or address
    </div>
  -->

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="mailto:contact@eit.edu.cn" data-tooltip="Email" data-style="bare" aria-label="Email">
      <i class="icon fa-solid fa-envelope"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://orcid.org/0000-0001-8713-9213" data-tooltip="ORCID" data-style="bare" aria-label="ORCID">
      <i class="icon fa-brands fa-orcid"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=ETJoidYAAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/https://github.com/eit-nlp-lab" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2025
    EIT-NLP
      |   Built with
    <a href="https://github.com/greenelab/lab-website-template">
      Lab Website Template
    </a>
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
